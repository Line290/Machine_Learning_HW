\documentclass[UTF8]{ctexart}
\title{ML Homework 1}
\author{林大权 \\ ID: 85610653}
\date{}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{bm}
\usepackage[colorlinks,linkcolor=red]{hyperref}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{subfloat}
\usepackage{indentfirst}
\leftmargin=0.25in
\oddsidemargin=0.25in
\textwidth=6.0in
\topmargin=-0.25in
\textheight=9.25in

\begin{document}
\maketitle

\begin{description}
\item[1 (1)]: 1) 反欺诈： 一些恶意商户为了在某个电商平台或者推荐平台上，刷自己旗下的商品的交易量或访问量，这时平台的安全人员可以用机器学习算法进行数据分析，找到离群点，很快发现欺诈数据。\\ 
2）交通拥堵预测：分析以往数据预测未来道路交通情况，及时规避拥堵路段。\\
\item[2 (2)]: 
\begin{proof}
Suppose a sufficiently small step $\alpha \bm{d}$, $\alpha > 0$.\\
We have $f(\bm{x}+\alpha\bm{d}) = f(\bm{x}) + \alpha \nabla f(\bm{x})^{T}\bm{d} + o(\alpha ||\bm{d}||)$, where $o(\alpha ||\bm{d}||)\rightarrow 0$ as $\alpha\rightarrow 0$, since $||\bm{d}||$ is $1$.\\
$\Rightarrow$
$$\frac{f(\bm{x}+\alpha\bm{d}) -f(\bm{x})}{\alpha} = \nabla f(\bm{x})^{T}\bm{d} + \frac{o(\alpha ||\bm{d}||)}{\alpha}$$
Since $\lim_{\alpha\to0}\frac{o(\alpha ||\bm{d}||)}{\alpha}$, \\
then $\exists \bar\alpha$, s.t. $\forall \alpha\in(0,\bar\alpha)$,\\
we have $\frac{o(\alpha ||\bm{d}||)}{\alpha} < \frac{1}{2}|\alpha \nabla f(\bm{x})^{T}\bm{d}|$\\
Since $\alpha \nabla f(\bm{x})^{T}\bm{d}<0$ by assumption, we conclude that $\forall \alpha\in(0,\bar\alpha)$,\\
$f(\bm{x}+\alpha\bm{d}) - f(\bm{x})< \frac{1}{2}\alpha \nabla f(\bm{x})^{T}\bm{d} < 0$\\
\end{proof}
\item[2 (1)]:
\begin{proof}
$\because$ the Hessian of $f:\bm{H}$is PSD, and $\forall \bm{x} \neq \bm{0}$, we have $\bm{x^{T}}\bm{H}\bm{x}\geq 0$\\
$\therefore$ From the key, we can get $f(\bm{x}) \geq f(\bm{x'})+\nabla f(\bm{x'})^{T}(\bm{x}-\bm{x'})$\\
$\therefore$ $\forall x,y \in \bm{dom}(f)$, Suppose $z = \lambda x +(1-\lambda) y$\\
we have 
$$f(\bm{x})\geq f(\bm{z}) + \nabla f(\bm{z})^T(\bm{x}-\bm{z}) \eqno{(1)}$$
$$f(\bm{y})\geq f(\bm{z}) + \nabla f(\bm{z})^T(\bm{y}-\bm{z}) \eqno{(2)}$$
then $(1)*\lambda+(2)*(1-\lambda)$, we have:
$$\lambda f(\bm{x}) + (1-\lambda)f(\bm{y}) \geq f(\bm{z}) + \nabla f(\bm{z})^T(\lambda \bm{x}+(1-\lambda)\bm{y} - \bm{z})=f(\bm{z})=f(\lambda \bm{x}+(1-\lambda)\bm{y})$$
$\therefore$ $f$ is convex.\\
\end{proof}
\item[2 (2)]:
\begin{proof}
$\because$ $f$ is convex, be definition:
$$\lambda f(\bm{y}) + (1-\lambda)f(\bm{x}) \geq f(\lambda \bm{y}+(1-\lambda)\bm{x}), \forall \lambda \in [0,1], x,y\in\bm{dom}(f)$$
$\Rightarrow$
$$f(\bm{x}) + \lambda(f(\bm{y})-f(\bm{x})) \geq f(\bm{x}+\lambda(\bm{y}-\bm{x}))$$
$\Rightarrow$
$$f(\bm{y})-f(\bm{x}) \geq \frac{f(\bm{x}+\lambda(\bm{y}-\bm{x})) - f(\bm{x})}{\lambda(\bm{y}-\bm{x})}(\bm{y} - \bm{x})$$
$\Rightarrow$
$$f(\bm{y})-f(\bm{x}) \geq \nabla f(\bm{x})^T(\bm{y}-\bm{x})$$

%add start
Suppose $\bm{x^*}$ is the global minimizer, $\bm{x^*}$ doesn't satisfy $\nabla f(\bm{x^*}) = 0$, we can move $\bm{x^*}$ along direction $-\nabla f(\bm{x^*})$ with non-zero distance to $\bm{y}$; such that $\nabla f(\bm{x^*})^T(\bm{y}-\bm{x^*}) < 0$.
\par Consider $\phi(\alpha) = f(\bm{x^*}+\alpha(\bm{y}-\bm{x^*}))$, because $f$ is convex, such that $\forall \alpha \in [0,1]$ $\bm{x^*} + \alpha(\bm{y}-\bm{x^*}) \in \bm{dom}(f)$.\\
Observe that

$$\phi\prime(\alpha) = (\bm{y}-\bm{x^*})^T \nabla f(\bm{x^*}+\alpha(\bm{y}-\bm{x^*}))$$

$\Rightarrow$ $$\phi\prime(0) = (\bm{y}-\bm{x^*})^T\nabla f(\bm{x^*}) < 0$$


This implies that

$$\exists \delta > 0, s.t.\ \phi(\alpha) < \phi(0), \forall \alpha \in (0, \delta)$$

$\Rightarrow$ $$f(\bm{x^*} + \alpha(\bm{y}-\bm{x^*})) < f(\bm{x^*}), \forall \alpha \in (0, \delta)$$
But this contradicts the optimality of $\bm{x^*}$.\\
%add end

\end{proof}

\item[3]: The cost function is:

$$y = a\bm{x} + b$$
$$J(a,b) = \frac{1}{2N}\sum_{i=0}^{N}(y_i - ax_i - b)^2$$

Termination criterion is number of iteration, here I set 50000.\\
Due to I rescale the input data, so the result:
$$loss = 0.26617$$
$$a = -0.206369981511$$
$$b = 5.44851173492$$
Shown in Pic.1
\begin{figure}
\centering
        \includegraphics[totalheight=8cm]{3.png}
    \caption{The result of 4-trainingdata.txt}
    \label{fig:verticalcell}
\end{figure}
\item[4]: The Huber cost function is:
\[
L_{\sigma}(y,f(x)) = 
\begin{cases}
\frac{1}{2}(y-f(x))^2 & |y-f(x)|\leq\sigma\\
\sigma |y-f(x)| - \frac{1}{2}\sigma^2 & otherwise
\end{cases}
\]
In 5-trainingdata.txt, use the cost function in Problem 3 is shown in Pic.2.\\
And loss 
\begin{figure}
\centering
        \includegraphics[totalheight=8cm]{5b.png}
    \caption{The result of 5-trainingdata.txt in Problem 3 cost function.}
    \label{fig:verticalcell}
\end{figure}
And the Huber cost funcion is Pic.3, a little improve than old one.

\begin{figure}
\centering
        \includegraphics[totalheight=8cm]{5a.png}
    \caption{The result of 5-trainingdata.txt in Huber cost function.}
    \label{fig:verticalcell}
\end{figure}
The tess loss for The least-square method and Huber method are 0.08336623884690787 and 0.08333359874471885, respectively.

\end{description}
\end{document}